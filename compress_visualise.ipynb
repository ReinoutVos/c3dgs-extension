{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Compression Pipeline:\n",
    "\n",
    "\n",
    "<div style=\"background-color: white; padding: 10px;\">\n",
    "    <img src=\"./docs/static/img/pipeline.svg\" alt=\"SVG Image\" width=\"1500px\" />\n",
    "</div>\n",
    "\n",
    "Instead of running the compression pipeline all at once, here you can run it step-by-step and explore the process."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## First simulate the command-line arguments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "simulated_args = [\n",
    "        \"--model_path\", \"./input_models/flower_hq\",\n",
    "        \"--data_device\", \"cuda\",\n",
    "        \"--output_vq\", \"./output\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# General\n",
    "import time\n",
    "import os\n",
    "from os import path\n",
    "from shutil import copyfile\n",
    "import gc\n",
    "import json\n",
    "from random import randint\n",
    "\n",
    "# Data\n",
    "import torch\n",
    "import numpy as np\n",
    "from tqdm import tqdm, trange\n",
    "\n",
    "# Visualisations\n",
    "from visualisation.plots import *\n",
    "\n",
    "# Arguments\n",
    "from argparse import ArgumentParser, Namespace\n",
    "from arguments import (\n",
    "    CompressionParams,\n",
    "    ModelParams,\n",
    "    OptimizationParams,\n",
    "    PipelineParams,\n",
    "    get_combined_args,\n",
    ")\n",
    "\n",
    "# c3dgs functions / classes\n",
    "from compress import unique_output_folder, calc_importance\n",
    "from gaussian_renderer import GaussianModel\n",
    "from scene import Scene\n",
    "from compression.vq import CompressionSettings\n",
    "from typing import Tuple\n",
    "from utils.splats import to_full_cov, extract_rot_scale\n",
    "from compression.vq import VectorQuantize, join_features\n",
    "from finetune import prepare_output_and_logger\n",
    "from gaussian_renderer import render\n",
    "from utils.loss_utils import l1_loss, ssim\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_arguments(simulated_args=[]):\n",
    "    # Initialize the argument parser\n",
    "    parser = ArgumentParser(description=\"Compression script parameters\")\n",
    "    \n",
    "    # Add the same argument groups as in the script\n",
    "    model = ModelParams(parser, sentinel=True)\n",
    "    model.data_device = \"cuda\"\n",
    "    pipeline = PipelineParams(parser)\n",
    "    op = OptimizationParams(parser)\n",
    "    comp = CompressionParams(parser)\n",
    "    \n",
    "    # Combine simulated args with parser arguments\n",
    "    args = get_combined_args(parser, simulated_args)\n",
    "    return args, model, pipeline, op, comp\n",
    "\n",
    "\n",
    "args, model, pipeline, op, comp = parse_arguments(simulated_args)\n",
    "\n",
    "# Set output folder if not specified\n",
    "if args.output_vq is None:\n",
    "    args.output_vq = unique_output_folder()\n",
    "\n",
    "# Extract parameters\n",
    "model_params = model.extract(args)\n",
    "optim_params = op.extract(args)\n",
    "pipeline_params = pipeline.extract(args)\n",
    "comp_params = comp.extract(args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the Gaussians\n",
    "gaussians = GaussianModel(\n",
    "    model_params.sh_degree, quantization=not optim_params.not_quantization_aware\n",
    ")\n",
    "\n",
    "# Initialize the scene (test cameras + train cameras)\n",
    "scene = Scene(\n",
    "    model_params, gaussians, load_iteration=comp_params.load_iteration, shuffle=True\n",
    ")\n",
    "\n",
    "# Load the Gaussians from the pre-trained model (checkpoint) into memory\n",
    "if comp_params.start_checkpoint:\n",
    "    (checkpoint_params, first_iter) = torch.load(comp_params.start_checkpoint)\n",
    "    gaussians.restore(checkpoint_params, optim_params)\n",
    "\n",
    "\n",
    "timings = {}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Parameter Sensitivity\n",
    "Note: The authors use 'sensitivity' and 'importance' interchangeably, this is very confusing I know "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Important hyperparameters                                     # Default value\n",
    "comp_params.color_importance_include                            # 0.6*1e-6\n",
    "comp_params.gaussian_importance_include                         # 0.3*1e-5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_time = time.time()\n",
    "\n",
    "color_importance, gaussian_sensitivity = calc_importance(\n",
    "    gaussians, scene, pipeline_params\n",
    ")\n",
    "end_time = time.time()\n",
    "timings[\"sensitivity_calculation\"] = end_time-start_time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "color_importance_include = torch.tensor(comp_params.color_importance_include)\n",
    "gaussian_importance_include = torch.tensor(comp_params.gaussian_importance_include)\n",
    "\n",
    "color_above_threshold = (color_importance > color_importance_include).sum().item()\n",
    "total_elements_color = color_importance.numel()\n",
    "\n",
    "gaussian_above_threshold = (gaussian_sensitivity > gaussian_importance_include).sum().item()\n",
    "total_elements_gaussian = gaussian_sensitivity.numel()\n",
    "\n",
    "color_threshold = 1.0 - (color_above_threshold / total_elements_color)\n",
    "gaussian_threshold = 1.0 - (gaussian_above_threshold / total_elements_gaussian)\n",
    "\n",
    "print(f\"Percentage of color_importance values below the threshold: {color_threshold * 100:.2f}%\")\n",
    "print(f\"Percentage of gaussian_importance values below the threshold: {gaussian_threshold * 100:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalize the tensors\n",
    "color_importance_norm = torch.nn.functional.normalize(color_importance.clone(), p=2).flatten()\n",
    "gaussian_sensitivity_norm = torch.nn.functional.normalize(gaussian_sensitivity.clone(), p=2).flatten()\n",
    "\n",
    "# # Normalize the tensors\n",
    "# color_importance_norm = color_importance / color_importance.max()\n",
    "# gaussian_sensitivity_norm = gaussian_sensitivity / gaussian_sensitivity.max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "color_shape_sensitivity_hist(color_importance_norm, gaussian_sensitivity_norm, color_threshold, gaussian_threshold)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Sensitivity-aware vector clustering\n",
    "Note: vector clustering = vector quantization = K-Means clustering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "------------------ Pruning ------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Important hyperparameters                                     # Default value\n",
    "comp_params.prune_threshold                                     # 0.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    start_time = time.time()\n",
    "\n",
    "    # Given a vector x ∈ R^D, we define its sensitivity as the maximum over its component’s sensitivity\n",
    "    color_importance_n = color_importance.amax(-1)\n",
    "    gaussian_importance_n = gaussian_sensitivity.amax(-1)\n",
    "\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "    # ------------------ Prune ------------------\n",
    "    prune_threshold = comp_params.prune_threshold\n",
    "\n",
    "    if prune_threshold >= 0:\n",
    "        non_prune_mask = color_importance_n > prune_threshold\n",
    "\n",
    "        # Get positions for each Gaussian\n",
    "        positions = gaussians.get_xyz\n",
    "        \n",
    "        # Separate the positions and sensitivities based on pruning mask\n",
    "        pos_keep = positions[non_prune_mask].cpu().numpy()\n",
    "        pos_prune = positions[~non_prune_mask].cpu().numpy()\n",
    "\n",
    "        gaussians.mask_splats(non_prune_mask)\n",
    "        gaussian_importance_n = gaussian_importance_n[non_prune_mask]\n",
    "        color_importance_n = color_importance_n[non_prune_mask]\n",
    "\n",
    "    end_time = time.time()\n",
    "    timings[\"pruning\"] = end_time-start_time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scatterplot_prune_gaussians(pos_prune, pos_keep, non_prune_mask)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "------------------ Color Compression ------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Important hyperparameters                                     # Default value\n",
    "comp_params.color_codebook_size                                 # 2**12\n",
    "# comp_params.color_cluster_iterations                            # 100\n",
    "comp_params.color_cluster_iterations = 1 # TODO: remove\n",
    "comp_params.color_decay                                         # 0.8\n",
    "comp_params.color_batch_size                                    # 2**18\n",
    "comp_params.color_compress_non_dir                              # True\n",
    "\n",
    "# Initialize the color codebook using parameters\n",
    "color_compression_settings = CompressionSettings(\n",
    "    codebook_size=comp_params.color_codebook_size,\n",
    "    importance_prune=comp_params.color_importance_prune,\n",
    "    importance_include=comp_params.color_importance_include,\n",
    "    steps=int(comp_params.color_cluster_iterations),\n",
    "    decay=comp_params.color_decay,\n",
    "    batch_size=comp_params.color_batch_size,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove zero sh component to get the color features\n",
    "if comp_params.color_compress_non_dir:\n",
    "    n_sh_coefs = gaussians.get_features.shape[1]\n",
    "    color_features = gaussians.get_features.detach().flatten(-2)\n",
    "else:\n",
    "    n_sh_coefs = gaussians.get_features.shape[1] - 1\n",
    "    color_features = gaussians.get_features[:, 1:].detach().flatten(-2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot the initial Color Feature Space:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_features_pca(color_features, label=\"Color Features\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vq_features_vis(\n",
    "    features: torch.Tensor,\n",
    "    importance: torch.Tensor,\n",
    "    codebook_size: int,\n",
    "    vq_chunk: int = 2**16,\n",
    "    steps: int = 1000,\n",
    "    decay: float = 0.8,\n",
    "    scale_normalize: bool = False,\n",
    ") -> Tuple[torch.Tensor, torch.Tensor]:\n",
    "    importance_n = importance/importance.max()\n",
    "    vq_model = VectorQuantize(\n",
    "        channels=features.shape[-1],\n",
    "        codebook_size=codebook_size,\n",
    "        decay=decay,\n",
    "    ).to(device=features.device)\n",
    "\n",
    "    vq_model.uniform_init(features)\n",
    "\n",
    "    errors = []\n",
    "    centroids_history = [vq_model.codebook.data.cpu().numpy().copy()] # Store the initial centroids\n",
    "\n",
    "    for i in trange(steps):\n",
    "        batch = torch.randint(low=0, high=features.shape[0], size=[vq_chunk])\n",
    "        vq_feature = features[batch]\n",
    "        error = vq_model.update(vq_feature, importance=importance_n[batch]).mean().item()\n",
    "        errors.append(error)\n",
    "\n",
    "        # Store centroids every 5th iteration\n",
    "        if (i + 1) % 5 == 0:\n",
    "            centroids_history.append(vq_model.codebook.data.cpu().numpy().copy())\n",
    "\n",
    "        if scale_normalize:\n",
    "            # this computes the trace of the codebook covariance matrices\n",
    "            # we devide by the trace to ensure that matrices have normalized eigenvalues / scales\n",
    "            tr = vq_model.codebook[:, [0, 3, 5]].sum(-1)\n",
    "            vq_model.codebook /= tr[:, None]\n",
    "\n",
    "    gc.collect()\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "    start = time.time()\n",
    "    _, vq_indices = vq_model(features)\n",
    "    torch.cuda.synchronize(device=vq_indices.device)\n",
    "    end = time.time()\n",
    "    print(f\"calculating indices took {end-start} seconds \")\n",
    "    return vq_model.codebook.data.detach(), vq_indices.detach(), errors, centroids_history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compress_color_vis(\n",
    "    gaussians: GaussianModel,\n",
    "    color_importance_n: torch.Tensor,\n",
    "    color_features: torch.Tensor,\n",
    "    color_comp: CompressionSettings,\n",
    "):\n",
    "    keep_mask = color_importance_n > color_comp.importance_include\n",
    "\n",
    "    print(f\"color keep: {keep_mask.float().mean()*100:.2f}%\")\n",
    "\n",
    "    vq_mask_c = ~keep_mask\n",
    "\n",
    "    if vq_mask_c.any():\n",
    "        color_codebook, color_vq_indices, errors, centroids_history = vq_features_vis(\n",
    "            color_features[vq_mask_c],\n",
    "            color_importance_n[vq_mask_c],\n",
    "            color_comp.codebook_size,\n",
    "            color_comp.batch_size,\n",
    "            color_comp.steps,\n",
    "        )\n",
    "    else:\n",
    "        color_codebook = torch.empty(\n",
    "            (0, color_features.shape[-1]), device=color_features.device\n",
    "        )\n",
    "        color_vq_indices = torch.empty(\n",
    "            (0,), device=color_features.device, dtype=torch.long\n",
    "        )\n",
    "\n",
    "    all_features = color_features\n",
    "    compressed_features, indices = join_features(\n",
    "        all_features, keep_mask, color_codebook, color_vq_indices\n",
    "    )\n",
    "\n",
    "    gaussians.set_color_indexed(compressed_features.reshape(-1, n_sh_coefs, 3), indices)\n",
    "\n",
    "    return errors, centroids_history,compressed_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "color_errors = []\n",
    "\n",
    "with torch.no_grad():\n",
    "\n",
    "    start_time = time.time()\n",
    "\n",
    "    color_comp = color_compression_settings if not comp_params.not_compress_color else None\n",
    "    if color_comp is not None:\n",
    "        color_errors, color_centroids_history, color_compressed_features = compress_color_vis(\n",
    "            gaussians,\n",
    "            color_importance_n,\n",
    "            color_features,\n",
    "            color_comp,\n",
    "        )\n",
    "\n",
    "    end_time = time.time()\n",
    "    timings[\"color clustering\"]=end_time-start_time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Animation of the centroid positions over time:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib notebook\n",
    "ani = animate_feature_clustering(color_features, color_centroids_history, title=\"2D Projection of Color Features\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Final Result - Initial Color Features (blue) vs Compressed Color Features (red):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "plot_features_and_compressed(color_features, color_compressed_features, title=\"2D Projection of All Features and Compressed Features\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_error_curve(color_errors)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "------------------ Gaussian Shape Compression ------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Important hyperparameters                                     # Default value\n",
    "comp_params.gaussian_codebook_size                              # 2**12\n",
    "# comp_params.gaussian_cluster_iterations                         # 800\n",
    "comp_params.gaussian_cluster_iterations = 1 # TODO: remove\n",
    "comp_params.gaussian_decay                                      # 0.8\n",
    "comp_params.gaussian_batch_size                                 # 2**20\n",
    "\n",
    "# Initialize the Gaussian shape codebook using parameters\n",
    "gaussian_compression_settings = CompressionSettings(\n",
    "    codebook_size=comp_params.gaussian_codebook_size,\n",
    "    importance_prune=None,\n",
    "    importance_include=comp_params.gaussian_importance_include,\n",
    "    steps=int(comp_params.gaussian_cluster_iterations),\n",
    "    decay=comp_params.gaussian_decay,\n",
    "    batch_size=comp_params.gaussian_batch_size,\n",
    ")\n",
    "\n",
    "\n",
    "# TODO: Experiment Gaussian importance prune"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For the Gaussian shape features, we use the normalized covariance matrix\n",
    "gaussian_shape_features = gaussians.get_normalized_covariance(strip_sym=True).detach()\n",
    "\n",
    "gaussian_shape_features_plot = gaussians.get_normalized_covariance(strip_sym=False).detach() # Symmetry required for matrix decomposition\n",
    "rot_plot, scale_plot = extract_rot_scale(gaussian_shape_features_plot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_features_pca(rot_plot)\n",
    "plot_features_pca(scale_plot)\n",
    "plot_features_3d(scale_plot, title=\"\", elev=40, azim=130)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compress_covariance_vis(\n",
    "    gaussians: GaussianModel,\n",
    "    gaussian_importance_n: torch.Tensor,\n",
    "    gaussian_shape_features: torch.Tensor,\n",
    "    gaussian_comp: CompressionSettings,\n",
    "):\n",
    "\n",
    "    keep_mask_g = gaussian_importance_n > gaussian_comp.importance_include\n",
    "\n",
    "    vq_mask_g = ~keep_mask_g\n",
    "\n",
    "    print(f\"gaussians keep: {keep_mask_g.float().mean()*100:.2f}%\")\n",
    "\n",
    "    if vq_mask_g.any():\n",
    "        cov_codebook, cov_vq_indices, errors, centroids_history = vq_features_vis(\n",
    "            gaussian_shape_features[vq_mask_g],\n",
    "            gaussian_importance_n[vq_mask_g],\n",
    "            gaussian_comp.codebook_size,\n",
    "            gaussian_comp.batch_size,\n",
    "            gaussian_comp.steps,\n",
    "            scale_normalize=True,\n",
    "        )\n",
    "    else:\n",
    "        cov_codebook = torch.empty(\n",
    "            (0, gaussian_shape_features.shape[1], 1), device=gaussian_shape_features.device\n",
    "        )\n",
    "        cov_vq_indices = torch.empty((0,), device=gaussian_shape_features.device, dtype=torch.long)\n",
    "\n",
    "    compressed_cov, cov_indices = join_features(\n",
    "        gaussian_shape_features,\n",
    "        keep_mask_g,\n",
    "        cov_codebook,\n",
    "        cov_vq_indices,\n",
    "    )\n",
    "\n",
    "    rot_vq, scale_vq = extract_rot_scale(to_full_cov(compressed_cov))\n",
    "\n",
    "    gaussians.set_gaussian_indexed(\n",
    "        rot_vq.to(compressed_cov.device),\n",
    "        scale_vq.to(compressed_cov.device),\n",
    "        cov_indices,\n",
    "    )\n",
    "    \n",
    "    return errors, centroids_history, to_full_cov(compressed_cov), rot_vq, scale_vq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shape_errors = []\n",
    "\n",
    "with torch.no_grad():\n",
    "\n",
    "    start_time = time.time()\n",
    "\n",
    "    gaussian_comp = gaussian_compression_settings if not comp_params.not_compress_gaussians else None\n",
    "    if gaussian_comp is not None:\n",
    "        shape_errors, shape_centroids_history, compressed_cov, rot_vq, scale_vq = compress_covariance_vis(\n",
    "            gaussians,\n",
    "            gaussian_importance_n,\n",
    "            gaussian_shape_features,\n",
    "            gaussian_comp,\n",
    "        )\n",
    "\n",
    "    end_time = time.time()\n",
    "    timings[\"shape clustering\"]=end_time-start_time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_centroids_to_rot_scale(centroids_history):\n",
    "    # Initialize lists to store rotation and scaling histories\n",
    "    rot_history = []\n",
    "    scale_history = []\n",
    "\n",
    "    for centroids in centroids_history:\n",
    "        # Convert centroids to tensor if they are not already\n",
    "        centroids_tensor = torch.tensor(centroids, device=\"cuda\") if not torch.is_tensor(centroids) else centroids\n",
    "        rot, scale = extract_rot_scale(to_full_cov(centroids_tensor))\n",
    "        rot_history.append(rot.cpu().numpy())  # Convert back to numpy if needed\n",
    "        scale_history.append(scale.cpu().numpy())\n",
    "\n",
    "    return rot_history, scale_history\n",
    "\n",
    "rot_history, scale_history = convert_centroids_to_rot_scale(shape_centroids_history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib notebook\n",
    "rot_history, scale_history = convert_centroids_to_rot_scale(shape_centroids_history)\n",
    "\n",
    "ani = animate_feature_clustering(rot_plot, rot_history, title=\"2D Projection of Gaussian Shape Features\")\n",
    "ani = animate_feature_clustering(scale_plot, scale_history, title=\"2D Projection of Gaussian Shape Features\")\n",
    "ani = animate_feature_clustering_3d(scale_plot, scale_history, title=\"2D Projection of Gaussian Shape Features\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "^ Shows the high redundancy in shape features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "plot_features_and_compressed(rot_plot, rot_vq)\n",
    "plot_features_and_compressed(scale_plot, scale_vq)\n",
    "plot_features_and_compressed_3d(scale_plot, scale_vq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "plot_error_curve(shape_errors)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before moving on to the finetuning step we have to prepare the output directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean up memory \n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "# Create output directory\n",
    "os.makedirs(comp_params.output_vq, exist_ok=True)\n",
    "\n",
    "# Copy configuration file\n",
    "copyfile(\n",
    "    path.join(model_params.model_path, \"cfg_args\"),\n",
    "    path.join(comp_params.output_vq, \"cfg_args\"),\n",
    ")\n",
    "\n",
    "# Update model path to point to new output directory\n",
    "model_params.model_path = comp_params.output_vq\n",
    "\n",
    "#  Save compression parameters in a new configuration file\n",
    "with open(\n",
    "    os.path.join(comp_params.output_vq, \"cfg_args_comp\"), \"w\"\n",
    ") as cfg_log_f:\n",
    "    cfg_log_f.write(str(Namespace(**vars(comp_params))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Quantization-Aware Fine-Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Important hyperparameters                                     # Default value\n",
    "comp_params.finetune_iterations = 200\n",
    "# comp_params.finetune_iterations                                 # 5000\n",
    "\n",
    "optim_params.lambda_dssim                                       # 0.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def finetune_vis(scene: Scene, vis_cam, dataset, opt, comp, pipe, testing_iterations, debug_from):\n",
    "    prepare_output_and_logger(comp.output_vq, dataset)\n",
    "\n",
    "    first_iter = scene.loaded_iter\n",
    "    max_iter = first_iter + comp.finetune_iterations\n",
    "\n",
    "    bg_color = [1, 1, 1] if dataset.white_background else [0, 0, 0]\n",
    "    background = torch.tensor(bg_color, dtype=torch.float32, device=\"cuda\")\n",
    "\n",
    "    iter_start = torch.cuda.Event(enable_timing=True)\n",
    "    iter_end = torch.cuda.Event(enable_timing=True)\n",
    "\n",
    "    scene.gaussians.training_setup(opt)\n",
    "    scene.gaussians.update_learning_rate(first_iter)\n",
    "\n",
    "    viewpoint_stack = None\n",
    "    ema_loss_for_log = 0.0\n",
    "    progress_bar = tqdm(range(first_iter, max_iter), desc=\"Training progress\")\n",
    "    first_iter += 1\n",
    "\n",
    "\n",
    "    # Store intermediate renderings for visualisation\n",
    "    rendering = render(vis_cam, scene.gaussians, pipe, background)[\"render\"]\n",
    "    rendered_images = [rendering.detach().cpu().numpy().transpose(1, 2, 0)]\n",
    "\n",
    "    losses = []\n",
    "\n",
    "\n",
    "    for iteration in range(first_iter, max_iter + 1):\n",
    "        iter_start.record()\n",
    "\n",
    "        # Pick a random Camera\n",
    "        if not viewpoint_stack:\n",
    "            viewpoint_stack = scene.getTrainCamerasExceptVis().copy()\n",
    "        viewpoint_cam = viewpoint_stack.pop(randint(0, len(viewpoint_stack) - 1))\n",
    "\n",
    "        # Render\n",
    "        if (iteration - 1) == debug_from:\n",
    "            pipe.debug = True\n",
    "        \n",
    "        render_pkg = render(viewpoint_cam, scene.gaussians, pipe, background)\n",
    "        image, viewspace_point_tensor, visibility_filter, radii = (\n",
    "            render_pkg[\"render\"],\n",
    "            render_pkg[\"viewspace_points\"],\n",
    "            render_pkg[\"visibility_filter\"],\n",
    "            render_pkg[\"radii\"],\n",
    "        )\n",
    "\n",
    "        # Loss\n",
    "        gt_image = viewpoint_cam.original_image.cuda()\n",
    "\n",
    "        Ll1 = l1_loss(image, gt_image)\n",
    "        loss = (1.0 - opt.lambda_dssim) * Ll1 + opt.lambda_dssim * (\n",
    "            1.0 - ssim(image, gt_image)\n",
    "        )\n",
    "        loss.backward()\n",
    "\n",
    "        losses.append(loss.detach().cpu())\n",
    "\n",
    "        iter_end.record()\n",
    "        scene.gaussians.update_learning_rate(iteration)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            # Progress bar\n",
    "            ema_loss_for_log = 0.4 * loss.item() + 0.6 * ema_loss_for_log\n",
    "            if iteration % 10 == 0:\n",
    "                progress_bar.set_postfix({\"Loss\": f\"{ema_loss_for_log:.{7}f}\"})\n",
    "                progress_bar.update(10)\n",
    "            if iteration == max_iter:\n",
    "                progress_bar.close()\n",
    "\n",
    "            # Optimizer step\n",
    "            if iteration < max_iter:\n",
    "                scene.gaussians.optimizer.step()\n",
    "                scene.gaussians.optimizer.zero_grad()\n",
    "\n",
    "            # Visualisation\n",
    "            if (iteration + 1) % 100 == 0:\n",
    "                rendering = render(vis_cam, scene.gaussians, pipe, background)[\"render\"]\n",
    "                rendered_images.append(rendering.detach().cpu().numpy().transpose(1, 2, 0))\n",
    "\n",
    "    \n",
    "    return rendered_images, losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iteration = scene.loaded_iter + comp_params.finetune_iterations\n",
    "\n",
    "if comp_params.finetune_iterations > 0:\n",
    "\n",
    "    start_time = time.time()\n",
    "\n",
    "    vis_cam = scene.getVisCamera()\n",
    "\n",
    "    rendered_images, losses = finetune_vis(\n",
    "        scene,\n",
    "        vis_cam,\n",
    "        model_params,\n",
    "        optim_params,\n",
    "        comp_params,\n",
    "        pipeline_params,\n",
    "        testing_iterations=[-1],\n",
    "        debug_from=-1,\n",
    "    )\n",
    "    \n",
    "    end_time = time.time()\n",
    "    timings[\"finetune\"]=end_time-start_time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib notebook\n",
    "ani = animate_training_renders(rendered_images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "     \n",
    "gt = vis_cam.original_image[0:3, :, :].unsqueeze(0)\n",
    "gt_np = gt.squeeze(0).permute(1, 2, 0).detach().cpu().numpy()\n",
    "\n",
    "draw_ground_truth_image(gt_np)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "window_size = 100\n",
    "\n",
    "plot_finetune_losses(losses, window_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Storage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_model_dir = \"input_models/flower_hq\"\n",
    "\n",
    "total_size = sum(os.path.getsize(os.path.join(input_model_dir, f)) for f in os.listdir(input_model_dir) if os.path.isfile(os.path.join(input_model_dir, f)))\n",
    "input_size = total_size / (1024 ** 2)\n",
    "print(f\"Total size of the input model: {input_size:.2f} MB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "out_file = path.join(\n",
    "    comp_params.output_vq,\n",
    "    f\"point_cloud/iteration_{iteration}/point_cloud.npz\",\n",
    ")\n",
    "\n",
    "start_time = time.time()\n",
    "gaussians.save_npz(out_file, sort_morton=not comp_params.not_sort_morton)\n",
    "end_time = time.time()\n",
    "\n",
    "timings[\"encode\"] = end_time-start_time\n",
    "timings[\"total\"] = sum(timings.values())\n",
    "\n",
    "with open(f\"{comp_params.output_vq}/times.json\",\"w\") as f:\n",
    "    json.dump(timings,f)\n",
    "file_size = os.path.getsize(out_file) / 1024**2\n",
    "print(f\"saved vq finetuned model to {out_file}\")\n",
    "print(f\"File size of the output model = {file_size:.2f}MB\")\n",
    "\n",
    "sizes = [input_size, file_size]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "visualise_timings(timings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "visualise_storage_metrics(sizes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from compress import render_and_eval\n",
    "\n",
    "metrics = render_and_eval(gaussians, scene, model_params, pipeline_params)\n",
    "metrics[\"size\"] = file_size\n",
    "print(metrics)\n",
    "with open(f\"{comp_params.output_vq}/results.json\",\"w\") as f:\n",
    "    json.dump({f\"ours_{iteration}\":metrics},f,indent=4)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "c3dgs_test",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
